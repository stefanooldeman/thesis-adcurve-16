Wat zijn de gepresenteerde oplossingen en waarom zijn deze volledig of niet?

In de volgende lijst worden nogmaals de functionele en niet functionele eisen nagelopen om te controleren of de gepresenteerde oplossingen ontwikkeld in Golang en Spark volledig zijn.

\begin{itemize}
    \item \textit{Kunnen de statistieken voor één webwinkel opnieuw worden gegenereerd?} Ja, doordat het ETL proces de data opsplitst in partities per webwinkel kan er een apart proces worden gestart per webwinkel. In Golang duurt maximaal 1 seconde en minimaal 200 milliseconden. In Spark duurt dit maximaal 37 seconden en minimaal 15 seconde.
    
    \item \textit{ Het creëren van data aggregaties voor alle webwinkels duurt niet langer dan 1 uur en 30 minuten.} Ja, dit is mogelijk. In Golang duurt dit gemiddeld 1 minuut en 22 seconden. In Spark duurt dit gemiddeld 3 minuten en 22 seconden.
    
    \item \textit{De kosten voor eventuele licenties en infrastructuur mogen niet hoger zijn dan dat voor de huidige oplossing.} De gebruikte hardware omschreven in \ref{subsec:hardware_specs} is slechts één machine. Het huidige MongoDB cluster bestaat uit meerdere machines. Daarnaast zijn er geen licentie kosten verbonden aan de gebruikte technologieën.
    
    \item \textit{Statistieken zijn altijd voor kantoor uren beschikbaar}. Afhankelijk van de tijd waarop publishers hun data beschikbaar stellen is dit mogelijk. Door de aggregatie een aantal keer te herhalen is er altijd data beschikbaar, afhankelijk van de data bronnen zijn de statistieken voor een webwinkel volledig. Voorbeeld: om 02:00 zijn alle orders en visits beschikbaar, op dit moment wordt er een aggregatie gestart voor alle shops. Vervolgens zijn de kosten voor een publisher beschikbaar om 03:00. Op dit moment wordt er opnieuw een aggregatie gestart voor alle shops die gebruik maken van deze publisher. Op deze wijze worden statistieken zo snel mogelijk beschikbaar gemaakt voor andere systemen.
    
    \item \textit{De gekozen oplossing heeft een productief programmeermodel en is afhankelijk van hardware architectuur}. Ja, dit blijkt uit onderzoek omschreven in  \ref{sec:gevonden_tools}
    
    \item \textit{De programmatuur is testbaar en hierdoor eenvoudig te onderhouden.}  Ja, dit blijkt uit onderzoek omschreven in  \ref{sec:gevonden_tools}
    
    \item \textit{Data aggregaties voor een groep webwinkels kunnen op een ander tijdstip worden geaggregeerd i.v.m. verschillende tijdszones}. Ja dit is mogelijk door een aantal processen achtereenvolgend te starten. In Golang zullen dit losse processen zijn. In Spark is het mogelijk om de data in één proces te verwerken zodat Spark de opdracht kan optimaliseren.
    
    \item \textit{De nieuwe oplossing moet schaalbaar zijn tot 10.000 webshops} Ja, dit blijkt uit onderzoek omschreven in \ref{sec:deelvraag2}. Daarnaast is het Golang proces te verdelen over meerdere machines door het gebruikt van ``Resque workers"\footnote{``Resque workers can be distributed between multiple machines, support priorities, are resilient to memory leaks" \parencite{github2016reque}} een methode die al wordt gebruikt binnen de organisatie. 
\end{itemize}

\subsection{Gevonden oplossing}
 
In het gegeven overzicht wordt duidelijk dat beide oplossingen voldoen aan alle functionele en niet functionele eisen. Daarom wordt geconcludeerd dat de gevonden oplossing volledig is bevonden. Het advies is om de oplossing in Golang verder door te ontwikkelen naar productie omdat uit onderzoek blijkt dat deze nog sneller is dan de implementatie in Spark.